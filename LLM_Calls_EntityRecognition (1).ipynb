{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203df698-17db-4549-a3bc-fa850bc67ad0",
   "metadata": {
    "id": "203df698-17db-4549-a3bc-fa850bc67ad0",
    "outputId": "9a1ebcc5-2b83-482f-c57a-630060ac9bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.68.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from openai) (4.8.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rachel\\anaconda3\\envs\\homl3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.68.2-py3-none-any.whl (606 kB)\n",
      "   ---------------------------------------- 0.0/606.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 606.1/606.1 kB 7.3 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-win_amd64.whl (208 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: jiter, h11, distro, httpcore, httpx, openai\n",
      "Successfully installed distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.9.0 openai-1.68.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aeaedb-fdde-4661-8ee3-188a2072399b",
   "metadata": {
    "id": "64aeaedb-fdde-4661-8ee3-188a2072399b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e5d71-7895-498f-8115-44c11ffd90b7",
   "metadata": {
    "id": "b19e5d71-7895-498f-8115-44c11ffd90b7"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8581c59-8dc6-4063-8dfc-d17f398f8d8c",
   "metadata": {
    "id": "f8581c59-8dc6-4063-8dfc-d17f398f8d8c"
   },
   "outputs": [],
   "source": [
    "# Prepare API key\n",
    "client = OpenAI(organization=\"xx\",\n",
    "               project=\"xx\",\n",
    "               api_key=\"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7081e19-b718-4b9a-901f-26066134e0bd",
   "metadata": {
    "id": "e7081e19-b718-4b9a-901f-26066134e0bd"
   },
   "outputs": [],
   "source": [
    "# Split mixed identities from a single note into separate entities through prompt\n",
    "prompt_role = \"\"\"\n",
    "You will be given a conversation between several parties. Your task is to parse the conversation and discern who said what in a conversation. Possible parties include the following:\n",
    "- BB (big brother);\n",
    "- BS (big sister);\n",
    "- LB (little brother);\n",
    "- LS (little sister);\n",
    "- PG (parents)\n",
    "- BIG (big brother and sister);\n",
    "- BBBS (big brother and big sister);\n",
    "- Little (little brother and sister);\n",
    "\n",
    "Note that these parties may be referred to by other labels (e.g., different capitalizations). The conversation may also include other parties not listed above. Please use your best judgment to determine who said what.\n",
    "\n",
    "Each conversation can have multiple parties saying multiple things. Make sure to extract all of them, not just the first one. Return your result as a JSON object with a key `utterances`, which is a list of all parsed speaker-text pairs.\n",
    "\n",
    "Also, some of the conversations may include event reminders or inviting them to fill out event forms. If you detect those sort of things, try to ignore them and focus on what the parties describe.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4375b",
   "metadata": {
    "id": "d9e4375b"
   },
   "outputs": [],
   "source": [
    "# indicate the output format as JSON\n",
    "output_format = {\n",
    "    \"format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"name\": \"role_identification\",\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"utterances\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"who\": {\"type\": \"string\"},\n",
    "                            \"what\": {\"type\": \"string\"}\n",
    "                        },\n",
    "                        \"required\": [\"who\", \"what\"],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"utterances\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4156a3d-8ca4-4cb5-b624-a31c2be3c32d",
   "metadata": {
    "id": "c4156a3d-8ca4-4cb5-b624-a31c2be3c32d"
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to call the model for a single conversation\n",
    "def extract_utterances_from_conversation(client, conversation: str, prompt_role: str, output_format: Dict) -> Dict:\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4o\",\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": prompt_role},\n",
    "                {\"role\": \"user\", \"content\": conversation}\n",
    "            ],\n",
    "            text=output_format,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        return json.loads(response.output_text)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Batch processing function\n",
    "def process_conversations_batch(client, conversations: List[str], output_path: str):\n",
    "    results = []\n",
    "    for convo in tqdm(conversations, desc=\"Processing conversations\"):\n",
    "        result = extract_utterances_from_conversation(client, convo, prompt_role, output_format)\n",
    "        results.append(result)\n",
    "\n",
    "    # Save all results to a JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n Saved {len(results)} processed results to: {output_path}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f955957-14f1-4ab7-b3e8-f3cb16d44375",
   "metadata": {
    "id": "1f955957-14f1-4ab7-b3e8-f3cb16d44375",
    "outputId": "b347f3b6-f6b9-4c9e-9f2b-8f5ef90e1c4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing conversations: 100%|████████████████████████████████████████████████████████| 50/50 [01:41<00:00,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved 50 processed results to: ./parsed_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data to extract call notes\n",
    "df = pd.read_excel(\"Training.xlsx\")\n",
    "df_sample = df.head(50)\n",
    "conversations = df_sample[\"Match Support Contact Notes\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "results = process_conversations_batch(\n",
    "    client=client,\n",
    "    conversations=conversations,\n",
    "    output_path=\"./parsed_results.json\"\n",
    ")\n",
    "\n",
    "# Save as Excel file\n",
    "flat_records = []\n",
    "for idx, item in enumerate(results):\n",
    "    if \"utterances\" in item:\n",
    "        for utter in item[\"utterances\"]:\n",
    "            flat_records.append({\n",
    "                \"Row\": idx,\n",
    "                \"Who\": utter.get(\"who\", \"\"),\n",
    "                \"What\": utter.get(\"what\", \"\")\n",
    "            })\n",
    "    else:\n",
    "        flat_records.append({\n",
    "            \"Row\": idx,\n",
    "            \"Who\": \"ERROR\",\n",
    "            \"What\": item.get(\"error\", \"Unknown error\")\n",
    "        })\n",
    "\n",
    "df_output = pd.DataFrame(flat_records)\n",
    "df_output.to_excel(\"./parsed_results.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
