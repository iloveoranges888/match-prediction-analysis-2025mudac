{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818a4231-f64c-4506-a8c7-ce27caa4e443",
   "metadata": {
    "id": "818a4231-f64c-4506-a8c7-ce27caa4e443"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the input features file\n",
    "df = pd.read_excel(\"Predicitve_Features_New.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1babd346-cb8b-415c-9718-ea958c8def2f",
   "metadata": {
    "id": "1babd346-cb8b-415c-9718-ea958c8def2f",
    "outputId": "5c8dfcc5-6a5c-4229-af6c-042be0489d36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RB MEHTA\\AppData\\Local\\Temp\\ipykernel_21984\\75902530.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(group_col, group_keys=False).apply(top_x_percent)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set your global parameter\n",
    "X_PERCENTILE = 100  # Top 30% or 100%\n",
    "\n",
    "# A safe copy of the original data\n",
    "original_df = df.copy()\n",
    "\n",
    "# Function to get top X% of rows for each match_id, sorted by completion_date\n",
    "def get_top_percentile_per_match(df, group_col, percentile):\n",
    "    def top_x_percent(group):\n",
    "        group = group.sort_values(by=\"Completion Date\")  # sort before slicing\n",
    "        group_size = len(group)\n",
    "        cutoff = int(np.ceil(group_size * percentile / 100))\n",
    "        return group.head(cutoff)\n",
    "\n",
    "    return df.groupby(group_col, group_keys=False).apply(top_x_percent)\n",
    "\n",
    "# Apply the function\n",
    "merged_df = get_top_percentile_per_match(original_df, group_col='Match ID 18Char', percentile=X_PERCENTILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7d57e8-018f-4bfb-95b8-128f47f2f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from Closure Reason to Closure Reason Category\n",
    "closure_reason_map = {\n",
    "    \"Agency: Challenges with program/partnership\": \"Incompatibility\",\n",
    "    \"Agency: Concern with Volunteer re: child safety\": \"Safety Concerns\",\n",
    "    \"Changing Match Type\": \"Miscommunications\",\n",
    "    \"Child/Family: Feels incompatible with volunteer\": \"Incompatibility\",\n",
    "    \"Child/Family: Infraction of match rules/agency policies\": \"Safety Concerns\",\n",
    "    \"Child/Family: Lost contact with agency\": \"Miscommunications\",\n",
    "    \"Child/Family: Lost contact with volunteer\": \"Miscommunications\",\n",
    "    \"Child/Family: Lost contact with volunteer/agency\": \"Miscommunications\",\n",
    "    \"Child/Family: Moved\": \"Neutral\",\n",
    "    \"Child/Family: Moved out of service area\": \"Neutral\",\n",
    "    \"Child/Family: Moved within service area\": \"Neutral\",\n",
    "    \"Child/Family: Time constraints\": \"Miscommunications\",\n",
    "    \"Child/Family: Unrealistic expectations\": \"Miscommunications\",\n",
    "    \"Child: Changed school/site\": \"Neutral\",\n",
    "    \"Child: Family structure changed\": \"Neutral\",\n",
    "    \"Child: Graduated\": \"Successes\",\n",
    "    \"Child: Lost interest\": \"Miscommunications\",\n",
    "    \"Child: Severity of challenges\": \"Neutral\",\n",
    "    \"COVID impact\": \"Neutral\",\n",
    "    \"Successful match closure\": \"Successes\",\n",
    "    \"Volunteer: Changed workplace/school partnership\": \"Neutral\",\n",
    "    \"Volunteer: Deceased\": \"Neutral\",\n",
    "    \"Volunteer: Feels incompatible with child/family\": \"Incompatibility\",\n",
    "    \"Volunteer: Health\": \"Neutral\",\n",
    "    \"Volunteer: Infraction of match rules/agency policies\": \"Safety Concerns\",\n",
    "    \"Volunteer: Lost contact with agency\": \"Miscommunications\",\n",
    "    \"Volunteer: Lost contact with child/agency\": \"Miscommunications\",\n",
    "    \"Volunteer: Lost contact with child/family\": \"Miscommunications\",\n",
    "    \"Volunteer: Moved\": \"Neutral\",\n",
    "    \"Volunteer: Moved out of service area\": \"Neutral\",\n",
    "    \"Volunteer: Moved within service area\": \"Neutral\",\n",
    "    \"Volunteer: Time constraint\": \"Miscommunications\",\n",
    "    \"Volunteer: Unrealistic expectations\": \"Miscommunications\",\n",
    "}\n",
    "\n",
    "# Add Closure Reason Category to merged_df using map\n",
    "merged_df['Closure Reason Category'] = merged_df['Closure Reason'].map(closure_reason_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f84df4-757f-4633-9e47-c428a79e7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_tokens_raw= {\n",
    "    'bb and lb', 'lb', 'bb', 'mec', 'big', 'bbbs', 'pg', 'pc', 'mc', 'narrator', 'observer', 'mss', 'bs', 'ls and bs',\n",
    "    'b_first_name', 'msc', 'kylie foss', 'mec julie', 'ls', 'laura phongsavath', 'l_first_name',\n",
    "    'b_first_name (likely bb or bs)', 'i (likely a coordinator or observer)', 'b_first_name (big)', 'kylie', 'rob',\n",
    "    'bs and ls', 'l_first_name and b_first_name', 'sb match support', 'allie', 'bs and mc', 'l_first_name and allie',\n",
    "    'olarian', 'little', \"lb's teacher\", 'b_first_name (bs)', 'match', 'katie', 'co-workers', 'as', 'uniqueka',\n",
    "    \"lb's grandma\", 'randy', 'unknown', 'randy and lb', 'you', 'little and big', 'dan', 'tmec', 'b_first_name b_last_name',\n",
    "    'mec and bs', 'big & little', 'meg', 'brenda (mec)', 'mom', 'dad', 'mary callahan schreiber (mec)',\n",
    "    'mary callahan schreiber', 'big/pg', 'josh (mec)', 'jessica (big)', 'rob (match support coordinator)', 'emily',\n",
    "    'grant', 'malik rucker', 'maypahou ly', 'olivia stillwell', 'both', 'big/little', 'mary', 'b_first_name muzik', 'alex',\n",
    "    'bbbs staff', 'event reminder', 'bc', 'bs and bb', 'mec and her supervisor', 'claudia garcia', 'ike b_last_name',\n",
    "    'big and little', 'chel', 'mary callahan schreiber (bbbs)', 'chel w (bs)', 'chel w', 'i', 'ad', 'mec and bb',\n",
    "    'pg and lb', 'mike', 'msc and pg', 'rob donahue', 'v', 'big or parent/guardian', 'katie culshaw', 'mec (katie)',\n",
    "    'mss notes', 'laura', 'dave', 'b_first_name w', 'match engagement coordinator notes', 'big sis', 'julie',\n",
    "    'big sister', 'lb and pg', 'cristina', 'lb and bb', 'cristina klappa', 'uniqueka smith', 'cristina (bbbs)',\n",
    "    'allison robbins', 'malik', 'nikki b_last_name', 'nikki and jeffery', 'mary and alex', 'alex and mary', 'matt',\n",
    "    'sara huffman', 'matt b_last_name', 'hawa fofana', 'joe', 'jeanna', 'kang (bbbs)', 'joe (bb)', 'kang', 'sara',\n",
    "    'jackie', 'l_first_name and jackie', \"ls' mom\", 'b_first_name (bb)', 'jake', 'pg and ls', 'ellie',\n",
    "    'ellie and l_first_name', 'pair', 'michelle', 'katlyn', 'dean pekarek', 'katelyn', 'lauren (bs)', 'lauren',\n",
    "    'bb or bs', 'tpc', 'dana', 'dana karls', 'kang moua', 'school contact', 'b_first_name defranco',\n",
    "    'micaela olson-macgregor', 'katie sobolewski', 'corinne castro', 'we', 'l_first_name & b_first_name',\n",
    "    'mec (match specialist)', 'b_first_name m', 'zarah augustine', 'olivia', 'hans', 'izzy', 'jeanna (bbbs)', 'angela',\n",
    "    'max', 'my parent/big- or- my little', 'unspecified', 'rob (bbbs)', 'tess haakonson', 'b_first_name rice', 'mai',\n",
    "    'sam', 'abby (bs)', 'email to pg and big', 'nick', 'little brother', 'danny', 'sam mihelich',\n",
    "    'rob donahue (bbbs)', 'pg, bb, and lb', 'coach', 'b_first_name and mario', 'pg (lisa)', 'd?meer and b_first_name',\n",
    "    \"d'meer\", 'heather', 'chue', 'program coordinator', 'jenna', 'gc', 'k and l_first_name', 'l_first_name and k', 'k',\n",
    "    \"ls's mom\", 'paula', 'jess', \"se'anna johnson\", 'big sister (b_first_name)', 'bbbs volunteer', 'sb',\n",
    "    'bs, ls, and pg', 'bs and pg', 'pc and bs', 'pc and b_first_name', 'guardian', 'sarah gibeaut',\n",
    "    'b_first_name (big sister)', 'bb/bs', 'andrea', 'rachel', 'b_first_name (bb or bs)', 'l_first_name (lb or ls)', 'mn',\n",
    "    'marissa', 'ben', 'bs and pc', 'mek', 'b_first_name r.', 'mary (coordinator)', 'mary (bbbs)', 'mary @ bbbs',\n",
    "    'taquana roberts', 'joseph b_last_name', 'staff', 'bigs', 'bb and mec', 'yi and nick (bbbs)', 'yi and nick', 'he',\n",
    "    'mario', 'nicki berger', 'b_first_name (bbbs)', 'pg, bs, and ls', 'group', 'pg and bb', 'tess', 'cindy',\n",
    "    'nylisha mcnutt', 'andrea (match engagement coordinator)', 'andrea (mec)', 'everyone', 'pg and little', 'andy',\n",
    "    'mario bradshaw', 'matches', 'kelly', 'cherie', 'karen depratto', 'ns', \"ls's family\", 'b_first_name and pc',\n",
    "    'bb and pc', 'pc and bb', 'vanessa', 'big and mec', 'bill', 'participant (bill)', 'mike sanford', 'kenzie b_last_name',\n",
    "    'keilly olsen', 'andrea (bbbs)', 'b_first_name b_last_name (big)', 'nate', 'bbs', 'bs and lb', 'mentor',\"Big\",\"Little\",\"big\",\"little\",'first_name','last_name'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a3a91a-f142-4f7d-afb6-db408ff60cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "blocked_tokens = set(re.sub(r\"[^a-zA-Z0-9_]+\", \"\", token.lower()) for token in blocked_tokens_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b716096e-f7f6-464c-936f-7b3037cc2122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING BAG OF WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [\n",
    "        lemmatizer.lemmatize(re.sub(r\"[^a-zA-Z0-9_]+\", \"\", word.lower()))\n",
    "        for word in tokens\n",
    "        if (\n",
    "            len(word) > 2 and\n",
    "            word.lower() not in blocked_tokens and\n",
    "            word.lower() not in stop_words and\n",
    "            word.lower() not in string.punctuation and\n",
    "            not word.isnumeric()\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "# Helper: Build exact n-gram only at level `n`\n",
    "def build_exact_ngrams(data_words, n, min_count=5, threshold=10):\n",
    "    if n < 2:\n",
    "        raise ValueError(\"n must be >= 2 for n-gram generation.\")\n",
    "    \n",
    "    base_words = data_words\n",
    "    for level in range(2, n + 1):\n",
    "        phrases = Phrases(base_words, min_count=min_count, threshold=threshold)\n",
    "        phraser = Phraser(phrases)\n",
    "        # At the final level, use the original cleaned tokens as base\n",
    "        base_words = [phraser[doc] for doc in data_words] if level == n else [phraser[doc] for doc in base_words]\n",
    "    # Filter to keep ONLY n-grams (e.g., contains '_' for bigrams/trigrams)\n",
    "    base_words = [[token for token in doc if \"_\" in token] for doc in base_words]\n",
    "    \n",
    "    return base_words\n",
    "\n",
    "# Main: Build topic model with clean, n-gram-enhanced text\n",
    "def topic_modeling_from_comments(df, ngram, num_topics, column_name=\"Comment_Dict\", passes=20, iterations=1000):\n",
    "    # Step 1: Tokenize and clean\n",
    "    data_words = df[column_name].dropna().apply(lambda x: x.split()).tolist()\n",
    "    data_words = [clean_tokens(doc) for doc in data_words]\n",
    "\n",
    "    # Step 2: Generate only specified n-gram level\n",
    "    data_words = build_exact_ngrams(data_words, n=ngram, min_count=15, threshold=10)\n",
    "\n",
    "    # Step 3: Dictionary and corpus for LDA\n",
    "    vocab_dict = corpora.Dictionary(data_words)\n",
    "    corpus = [vocab_dict.doc2bow(text) for text in data_words]\n",
    "\n",
    "    # Step 4: Train LDA model\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=vocab_dict,\n",
    "        num_topics=num_topics,\n",
    "        random_state=1,\n",
    "        chunksize=1000,\n",
    "        passes=passes,\n",
    "        iterations=iterations\n",
    "    )\n",
    "    # Step 5: Compute coherence score\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=data_words, dictionary=vocab_dict, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    return lda_model, corpus, vocab_dict, coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52c0970d-c74a-4994-8d35-fb88ff6c9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING TIF-DF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from gensim.models import Phrases, TfidfModel\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# Load English stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [\n",
    "        lemmatizer.lemmatize(re.sub(r\"[^a-zA-Z0-9_]+\", \"\", word.lower()))\n",
    "        for word in tokens\n",
    "        if (\n",
    "            len(word) > 2 and\n",
    "            word.lower() not in blocked_tokens and\n",
    "            word.lower() not in stop_words and\n",
    "            word.lower() not in string.punctuation and\n",
    "            not word.isnumeric()\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Build n-gram only at level `n` and filter to keep only those\n",
    "def build_exact_ngrams(data_words, n, min_count=5, threshold=10):\n",
    "    if n < 2:\n",
    "        raise ValueError(\"n must be >= 2 for n-gram generation.\")\n",
    "    \n",
    "    base_words = data_words\n",
    "    for level in range(2, n + 1):\n",
    "        phrases = Phrases(base_words, min_count=min_count, threshold=threshold)\n",
    "        phraser = Phraser(phrases)\n",
    "        base_words = [phraser[doc] for doc in data_words] if level == n else [phraser[doc] for doc in base_words]\n",
    "    \n",
    "    # ✅ Only keep tokens that are actual n-grams AND not in blocked list\n",
    "    base_words = [\n",
    "        [token for token in doc if \"_\" in token and token not in blocked_tokens]\n",
    "        for doc in base_words\n",
    "    ]\n",
    "\n",
    "    return base_words\n",
    "\n",
    "# Main topic modeling function\n",
    "def topic_modeling_from_comments_tifdf(df, ngram, num_topics, column_name=\"Comment_Dict\", passes=20, iterations=1000, use_tfidf=False):\n",
    "    # Step 1: Tokenize and clean\n",
    "    data_words = df[column_name].dropna().apply(lambda x: x.split()).tolist()\n",
    "    data_words = [clean_tokens(doc) for doc in data_words]\n",
    "\n",
    "    # Step 2: Generate only specified n-gram level\n",
    "    data_words = build_exact_ngrams(data_words, n=ngram, min_count=10, threshold=20)\n",
    "\n",
    "    # Step 3: Dictionary and corpus\n",
    "    vocab_dict = corpora.Dictionary(data_words)\n",
    "    corpus = [vocab_dict.doc2bow(text) for text in data_words]\n",
    "\n",
    "    # Optional TF-IDF weighting\n",
    "    if use_tfidf:\n",
    "        tfidf = TfidfModel(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "\n",
    "    # Step 4: Train LDA model\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=vocab_dict,\n",
    "        num_topics=num_topics,\n",
    "        random_state=1,\n",
    "        chunksize=1000,\n",
    "        passes=passes,\n",
    "        iterations=iterations\n",
    "    )\n",
    "\n",
    "    # Step 5: Compute coherence score\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=data_words, dictionary=vocab_dict, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    return lda_model, corpus, vocab_dict, coherence_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f35ae6b-7c54-4ea9-8ac4-5910a1862389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter merged_df to match the corpus rows (i.e., non-null and tokenized)\n",
    "valid_comments_df = merged_df[merged_df[\"Comment_Dict\"].notna()].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd1f220b-8657-4706-b77a-bc1213bcdc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model, corpus, vocab_dict, coherence_score = topic_modeling_from_comments_tifdf(\n",
    "    valid_comments_df,\n",
    "    ngram=2,\n",
    "    num_topics=5,\n",
    "    use_tfidf=True  # Turn this on\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caf7fc7a-3374-45a5-a4f8-8eab327c5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Top 10 keywords per topic:\n",
      "\n",
      "[(0,\n",
      "  '0.063*\"discus_further\" + 0.055*\"safety_reminder\" + 0.055*\"read_understand\" '\n",
      "  '+ 0.038*\"rob_hey\" + 0.034*\"sport_buddy\" + 0.020*\"open_gym\" + '\n",
      "  '0.014*\"play_basketball\" + 0.013*\"reaching_out\" + 0.011*\"understand_rule\" + '\n",
      "  '0.009*\"viking_tailgate\"'),\n",
      " (1,\n",
      "  '0.028*\"gotten_together\" + 0.018*\"typically_spend\" + '\n",
      "  '0.016*\"approximately_many\" + 0.016*\"how_long\" + 0.014*\"23_hour\" + '\n",
      "  '0.014*\"mentoring_hard\" + 0.013*\"make_sense\" + 0.013*\"saint_game\" + '\n",
      "  '0.013*\"safety_reminder\" + 0.012*\"winter_picnic\"'),\n",
      " (2,\n",
      "  '0.020*\"andy_hi\" + 0.020*\"warmly_andy\" + 0.020*\"let_know\" + '\n",
      "  '0.020*\"glad_hear\" + 0.019*\"thank_filling\" + 0.016*\"check_in\" + '\n",
      "  '0.015*\"stay_connected\" + 0.014*\"hello_b_first_name\" + 0.011*\"feel_free\" + '\n",
      "  '0.010*\"hi_b_first_name\"'),\n",
      " (3,\n",
      "  '0.054*\"let_know\" + 0.019*\"hey_b_first_name\" + 0.016*\"make_sure\" + '\n",
      "  '0.015*\"hi_b_first_name\" + 0.015*\"looking_forward\" + 0.014*\"anything_else\" + '\n",
      "  '0.014*\"come_up\" + 0.012*\"look_forward\" + 0.012*\"l_first_names\" + '\n",
      "  '0.011*\"right_now\"'),\n",
      " (4,\n",
      "  '0.015*\"called_checkin\" + 0.015*\"pen_pal\" + 0.013*\"keep_updated\" + '\n",
      "  '0.012*\"winter_break\" + 0.011*\"monthly_newsletter\" + 0.010*\"acuity_answered\" '\n",
      "  '+ 0.010*\"build_relationship\" + 0.009*\"holiday_season\" + 0.008*\"meet_ups\" + '\n",
      "  '0.008*\"ate_lunch\"')]\n",
      "\n",
      "🔹 Top 20 keywords per topic:\n",
      "\n",
      "[(0,\n",
      "  '0.063*\"discus_further\" + 0.055*\"safety_reminder\" + 0.055*\"read_understand\" '\n",
      "  '+ 0.038*\"rob_hey\" + 0.034*\"sport_buddy\" + 0.020*\"open_gym\" + '\n",
      "  '0.014*\"play_basketball\" + 0.013*\"reaching_out\" + 0.011*\"understand_rule\" + '\n",
      "  '0.009*\"viking_tailgate\" + 0.009*\"bbbs_staff\" + 0.008*\"next_step\" + '\n",
      "  '0.008*\"via_text\" + 0.008*\"mall_america\" + 0.007*\"dog_park\" + '\n",
      "  '0.007*\"tim_sinnett\" + 0.007*\"interacted_positively\" + 0.006*\"not_yet\" + '\n",
      "  '0.006*\"vision_board\" + 0.006*\"let_know\"'),\n",
      " (1,\n",
      "  '0.028*\"gotten_together\" + 0.018*\"typically_spend\" + '\n",
      "  '0.016*\"approximately_many\" + 0.016*\"how_long\" + 0.014*\"23_hour\" + '\n",
      "  '0.014*\"mentoring_hard\" + 0.013*\"make_sense\" + 0.013*\"saint_game\" + '\n",
      "  '0.013*\"safety_reminder\" + 0.012*\"winter_picnic\" + 0.011*\"12_hour\" + '\n",
      "  '0.011*\"read_understand\" + 0.011*\"youth_goal\" + 0.011*\"ice_cream\" + '\n",
      "  '0.010*\"coming_along\" + 0.009*\"attended_hangout\" + 0.009*\"how_satisfied\" + '\n",
      "  '0.009*\"bucket_list\" + 0.009*\"ice_skating\" + 0.009*\"34_hour\"'),\n",
      " (2,\n",
      "  '0.020*\"andy_hi\" + 0.020*\"warmly_andy\" + 0.020*\"let_know\" + '\n",
      "  '0.020*\"glad_hear\" + 0.019*\"thank_filling\" + 0.016*\"check_in\" + '\n",
      "  '0.015*\"stay_connected\" + 0.014*\"hello_b_first_name\" + 0.011*\"feel_free\" + '\n",
      "  '0.010*\"hi_b_first_name\" + 0.010*\"thank_helping\" + 0.010*\"brother_sister\" + '\n",
      "  '0.009*\"hey_b_first_name\" + 0.009*\"thanks_checking\" + '\n",
      "  '0.009*\"wonderful_holiday\" + 0.009*\"heard_back\" + 0.009*\"good_morning\" + '\n",
      "  '0.009*\"newer_one\" + 0.009*\"read_understand\" + 0.009*\"reminder_requires\"'),\n",
      " (3,\n",
      "  '0.054*\"let_know\" + 0.019*\"hey_b_first_name\" + 0.016*\"make_sure\" + '\n",
      "  '0.015*\"hi_b_first_name\" + 0.015*\"looking_forward\" + 0.014*\"anything_else\" + '\n",
      "  '0.014*\"come_up\" + 0.012*\"look_forward\" + 0.012*\"l_first_names\" + '\n",
      "  '0.011*\"right_now\" + 0.010*\"letting_know\" + 0.009*\"one_another\" + '\n",
      "  '0.009*\"hang_out\" + 0.009*\"thanks_b_first_name\" + 0.009*\"b_last_name\" + '\n",
      "  '0.008*\"take_care\" + 0.008*\"conversation_starter\" + 0.008*\"give_call\" + '\n",
      "  '0.008*\"phone_number\" + 0.008*\"liked_idea\"'),\n",
      " (4,\n",
      "  '0.015*\"called_checkin\" + 0.015*\"pen_pal\" + 0.013*\"keep_updated\" + '\n",
      "  '0.012*\"winter_break\" + 0.011*\"monthly_newsletter\" + 0.010*\"acuity_answered\" '\n",
      "  '+ 0.010*\"build_relationship\" + 0.009*\"holiday_season\" + 0.008*\"meet_ups\" + '\n",
      "  '0.008*\"ate_lunch\" + 0.008*\"checked_doing\" + 0.007*\"bike_ride\" + '\n",
      "  '0.007*\"take_advantage\" + 0.007*\"cant_wait\" + 0.007*\"favorite_worksheet\" + '\n",
      "  '0.007*\"make_sure\" + 0.007*\"trampoline_park\" + 0.006*\"escape_room\" + '\n",
      "  '0.006*\"supportactivity_needed\" + 0.006*\"sculpture_garden\"')]\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the topics\n",
    "from pprint import pprint\n",
    "\n",
    "# 🔟 Print top 10 keywords per topic\n",
    "print(\"🔹 Top 10 keywords per topic:\\n\")\n",
    "pprint(lda_model.print_topics(num_topics=10, num_words=10))\n",
    "\n",
    "# 🟢 Print top 20 keywords per topic\n",
    "print(\"\\n🔹 Top 20 keywords per topic:\\n\")\n",
    "pprint(lda_model.print_topics(num_topics=10, num_words=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0895236-1c94-47a8-9b22-6c01fb75e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_topic_distribution(lda_model, corpus, num_topics):\n",
    "    all_distributions = []\n",
    "    for doc in corpus:\n",
    "        topic_probs = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        topic_vector = np.array([prob for _, prob in sorted(topic_probs)])\n",
    "        all_distributions.append(topic_vector)\n",
    "    return np.array(all_distributions)\n",
    "\n",
    "topic_distributions = get_topic_distribution(lda_model, corpus, num_topics=10)\n",
    "\n",
    "\n",
    "\n",
    "# Then join\n",
    "topic_df = pd.DataFrame(topic_distributions, columns=[f\"Topic_{i}\" for i in range(topic_distributions.shape[1])])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50ee2c7e-3a03-4588-b9e1-78ad45f1db48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e10940c6-6566-46b7-8700-61cff433080b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row', 'Match ID 18Char', 'Completion Date', 'Comment_Dict',\n",
       "       'Avg_Cadence_Days', 'Call Frequency', 'Stage', 'Little ID', 'Big ID',\n",
       "       'Big County', 'Big Age', 'Big Gender', 'Big Race Category',\n",
       "       'Big Occupation Category', 'Little Gender', 'Little Race Category',\n",
       "       'Little Closure Age', 'Match Activation Date', 'Match Length',\n",
       "       'Closure Reason', 'Unresponsiveness_Mentioned',\n",
       "       'External_Stressors_Mentioned', 'Conflict_Language_Mentioned',\n",
       "       'Future_Oriented_Language', 'Shared_Interest',\n",
       "       'Communication_Support_Positive', 'Conflict_concerns_policy_flag',\n",
       "       'Relationship_quality_positive', 'Shared_interest_count',\n",
       "       'Engagement_consistency_positive', 'Mental_health_growth_positive',\n",
       "       'Sense_of_Growth', 'Child_Safety_Concern', 'Closure Reason Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_comments_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3576b8a-a4f5-43e7-9880-2a38c8501fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = valid_comments_df.join(topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3bd8b814-38ce-4858-bb13-9a2afc35af85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row', 'Match ID 18Char', 'Completion Date', 'Comment_Dict',\n",
       "       'Avg_Cadence_Days', 'Call Frequency', 'Stage', 'Little ID', 'Big ID',\n",
       "       'Big County', 'Big Age', 'Big Gender', 'Big Race Category',\n",
       "       'Big Occupation Category', 'Little Gender', 'Little Race Category',\n",
       "       'Little Closure Age', 'Match Activation Date', 'Match Length',\n",
       "       'Closure Reason', 'Unresponsiveness_Mentioned',\n",
       "       'External_Stressors_Mentioned', 'Conflict_Language_Mentioned',\n",
       "       'Future_Oriented_Language', 'Shared_Interest',\n",
       "       'Communication_Support_Positive', 'Conflict_concerns_policy_flag',\n",
       "       'Relationship_quality_positive', 'Shared_interest_count',\n",
       "       'Engagement_consistency_positive', 'Mental_health_growth_positive',\n",
       "       'Sense_of_Growth', 'Child_Safety_Concern', 'Closure Reason Category',\n",
       "       'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "564fca20-5493-4937-beda-6d1c41b1f9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37619, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2ffd0bd-ec7b-47a0-a587-95be38e25a3f",
   "metadata": {
    "id": "c2ffd0bd-ec7b-47a0-a587-95be38e25a3f",
    "outputId": "49a3a2c8-f086-46eb-84d0-9606a4800916"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RB MEHTA\\AppData\\Local\\Temp\\ipykernel_21984\\2058297198.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result_df = merged_df.groupby(\"Match ID 18Char\").apply(calculate_days).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Sort by match_id and completion_date\n",
    "merged_df = merged_df.sort_values(by=['Match ID 18Char', 'Completion Date'])\n",
    "\n",
    "# Calculate days since last call\n",
    "def calculate_days(group):\n",
    "    group = group.sort_values(by=\"Completion Date\")\n",
    "    group[\"days_since_last_call\"] = group[\"Completion Date\"].diff().dt.days\n",
    "    # For the first call, use Match Activation Date\n",
    "    group.iloc[0, group.columns.get_loc(\"days_since_last_call\")] = (\n",
    "        group.iloc[0][\"Completion Date\"] - group.iloc[0][\"Match Activation Date\"]\n",
    "    ).days\n",
    "    return group\n",
    "\n",
    "result_df = merged_df.groupby(\"Match ID 18Char\").apply(calculate_days).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1849c9d4-618c-4b14-89b1-5eb6d9b27940",
   "metadata": {
    "id": "1849c9d4-618c-4b14-89b1-5eb6d9b27940"
   },
   "outputs": [],
   "source": [
    "# Create a separate DataFrame with only match_id and all features\n",
    "#Call Frequency (Numericak) - Average\n",
    "grouped_df = merged_df.groupby(\"Match ID 18Char\")[\"Call Frequency\"].count().reset_index()\n",
    "grouped_df.rename(columns={\"Call Frequency\": \"avg_call_frequency_days\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "534497c7-b5be-4b65-915a-013a8d6e880e",
   "metadata": {
    "id": "534497c7-b5be-4b65-915a-013a8d6e880e"
   },
   "outputs": [],
   "source": [
    "#Cadence Days (Numerical)\n",
    "# Compute stats from days_since_last_call grouped by match_id\n",
    "call_stats = result_df.groupby(\"Match ID 18Char\")[\"days_since_last_call\"].agg(\n",
    "    std_days=\"std\",\n",
    "    var_days=\"var\",\n",
    "    min_days=\"min\",\n",
    "    max_days=\"max\",\n",
    "    mean_days=\"mean\"\n",
    ").reset_index()\n",
    "\n",
    "# Merge this with the average call frequency dataframe\n",
    "final_df = pd.merge(grouped_df, call_stats, on=\"Match ID 18Char\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "570e70f1-b93b-4987-b416-ce0fa6512413",
   "metadata": {
    "id": "570e70f1-b93b-4987-b416-ce0fa6512413"
   },
   "outputs": [],
   "source": [
    "#Shared Interest Count (Numerical)\n",
    "# Basic stats: std, var, min, max, mean\n",
    "interest_stats = result_df.groupby(\"Match ID 18Char\")[\"Shared_interest_count\"].agg(\n",
    "    std_shared=\"std\",\n",
    "    var_shared=\"var\",\n",
    "    min_shared=\"min\",\n",
    "    max_shared=\"max\",\n",
    "    mean_shared=\"mean\"\n",
    ").reset_index()\n",
    "\n",
    "# Merge stats + mode for Shared_interest_count\n",
    "final_df = pd.merge(final_df, interest_stats, on=\"Match ID 18Char\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9450b181-09db-4bb7-9295-ab83a85511bd",
   "metadata": {
    "id": "9450b181-09db-4bb7-9295-ab83a85511bd"
   },
   "outputs": [],
   "source": [
    "#Child safet Concern and Conflict concerns policy flag\n",
    "# Ensure binary columns are in numeric 0/1 format\n",
    "result_df[\"Child_Safety_Concern\"] = pd.to_numeric(result_df[\"Child_Safety_Concern\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "result_df[\"Conflict_concerns_policy_flag\"] = pd.to_numeric(result_df[\"Conflict_concerns_policy_flag\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Group by match_id and set flag to 1 if it appears at least once\n",
    "binary_flags = result_df.groupby(\"Match ID 18Char\")[[\"Child_Safety_Concern\", \"Conflict_concerns_policy_flag\"]].max().reset_index()\n",
    "\n",
    "# Merge into final_df\n",
    "final_df = final_df.merge(binary_flags, on=\"Match ID 18Char\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b7a194f-d2d0-4d74-bab2-10b87f522269",
   "metadata": {
    "id": "1b7a194f-d2d0-4d74-bab2-10b87f522269"
   },
   "outputs": [],
   "source": [
    "# List of columns to calculate mean\n",
    "mean_columns = [\n",
    "    \"Engagement_consistency_positive\",\n",
    "    \"Mental_health_growth_positive\",\n",
    "    \"Sense_of_Growth\",\n",
    "    \"Relationship_quality_positive\",\n",
    "    \"Unresponsiveness_Mentioned\",\n",
    "    \"External_Stressors_Mentioned\",\n",
    "    \"Conflict_Language_Mentioned\",\n",
    "    \"Future_Oriented_Language\",\n",
    "    \"Shared_Interest\",\n",
    "    \"Communication_Support_Positive\",\n",
    "    \"Conflict_concerns_policy_flag\",\n",
    "    'Topic_0', 'Topic_1',\n",
    "       'Topic_2', 'Topic_3', 'Topic_4'\n",
    "    \n",
    "]\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "for col in mean_columns:\n",
    "    result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "\n",
    "# Group by match_id and calculate the mean\n",
    "mean_df = result_df.groupby(\"Match ID 18Char\")[mean_columns].mean().reset_index()\n",
    "\n",
    "# Rename the mean of Conflict_concerns_policy_flag\n",
    "mean_df.rename(columns={\"Conflict_concerns_policy_flag\": \"Conflict_concerns_policy_flag_prop\"}, inplace=True)\n",
    "\n",
    "# Merge into final_df\n",
    "final_df = final_df.merge(mean_df, on=\"Match ID 18Char\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fa632c5-e769-4054-9fff-3b2d8118bb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3247, 17)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5c1ab56-5002-46f6-89c7-de3aa93d529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Match ID 18Char', 'avg_call_frequency_days', 'std_days', 'var_days',\n",
       "       'min_days', 'max_days', 'mean_days', 'std_shared', 'var_shared',\n",
       "       'min_shared', 'max_shared', 'mean_shared', 'Child_Safety_Concern',\n",
       "       'Conflict_concerns_policy_flag', 'Engagement_consistency_positive',\n",
       "       'Mental_health_growth_positive', 'Sense_of_Growth',\n",
       "       'Relationship_quality_positive', 'Unresponsiveness_Mentioned',\n",
       "       'External_Stressors_Mentioned', 'Conflict_Language_Mentioned',\n",
       "       'Future_Oriented_Language', 'Shared_Interest',\n",
       "       'Communication_Support_Positive', 'Conflict_concerns_policy_flag_prop',\n",
       "       'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57bc20d9-e3d6-47d6-af81-43986d82ad3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3247, 30)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dddd3ece-8ae9-4e95-beef-3dc3f49ba56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row', 'Match ID 18Char', 'Completion Date', 'Comment_Dict',\n",
       "       'Avg_Cadence_Days', 'Call Frequency', 'Stage', 'Little ID', 'Big ID',\n",
       "       'Big County', 'Big Age', 'Big Gender', 'Big Race Category',\n",
       "       'Big Occupation Category', 'Little Gender', 'Little Race Category',\n",
       "       'Little Closure Age', 'Match Activation Date', 'Match Length',\n",
       "       'Closure Reason', 'Unresponsiveness_Mentioned',\n",
       "       'External_Stressors_Mentioned', 'Conflict_Language_Mentioned',\n",
       "       'Future_Oriented_Language', 'Shared_Interest',\n",
       "       'Communication_Support_Positive', 'Conflict_concerns_policy_flag',\n",
       "       'Relationship_quality_positive', 'Shared_interest_count',\n",
       "       'Engagement_consistency_positive', 'Mental_health_growth_positive',\n",
       "       'Sense_of_Growth', 'Child_Safety_Concern', 'Closure Reason Category',\n",
       "       'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4',\n",
       "       'days_since_last_call'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e9535b1-0e4c-4256-871d-0087bdaefcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row', 'Match ID 18Char', 'Completion Date', 'Comment_Dict',\n",
       "       'Avg_Cadence_Days', 'Call Frequency', 'Stage', 'Little ID', 'Big ID',\n",
       "       'Big County', 'Big Age', 'Big Gender', 'Big Race Category',\n",
       "       'Big Occupation Category', 'Little Gender', 'Little Race Category',\n",
       "       'Little Closure Age', 'Match Activation Date', 'Match Length',\n",
       "       'Closure Reason', 'Unresponsiveness_Mentioned',\n",
       "       'External_Stressors_Mentioned', 'Conflict_Language_Mentioned',\n",
       "       'Future_Oriented_Language', 'Shared_Interest',\n",
       "       'Communication_Support_Positive', 'Conflict_concerns_policy_flag',\n",
       "       'Relationship_quality_positive', 'Shared_interest_count',\n",
       "       'Engagement_consistency_positive', 'Mental_health_growth_positive',\n",
       "       'Sense_of_Growth', 'Child_Safety_Concern', 'Closure Reason Category',\n",
       "       'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31784547-0697-4dd6-a894-768deb8a1473",
   "metadata": {
    "id": "31784547-0697-4dd6-a894-768deb8a1473"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18b5e613-2731-4713-ae77-f076a1461653",
   "metadata": {
    "id": "18b5e613-2731-4713-ae77-f076a1461653"
   },
   "outputs": [],
   "source": [
    "merged_df['Big Occupation Category'] = merged_df['Big Occupation Category'].fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62724af8-19aa-4b51-b0b5-7c4e5e7df6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3247, 41)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to merge (include the key for merging)\n",
    "cols_to_merge = [\n",
    "    'Match ID 18Char',  # join key\n",
    "    'Big County', 'Big Age', 'Big Gender', 'Big Race Category',\n",
    "    'Big Occupation Category', 'Little Gender', 'Little Race Category',\n",
    "    'Little Closure Age','Closure Reason Category','Closure Reason','Stage'\n",
    "]\n",
    "\n",
    "demographics_subset = merged_df[cols_to_merge].drop_duplicates(subset='Match ID 18Char')\n",
    "\n",
    "# Merge with final_df\n",
    "top_data = pd.merge(final_df, demographics_subset, on='Match ID 18Char', how='left')\n",
    "\n",
    "# Preview the result\n",
    "top_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d5620dc-6508-478c-a92d-160643be1f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Match ID 18Char', 'avg_call_frequency_days', 'std_days', 'var_days',\n",
       "       'min_days', 'max_days', 'mean_days', 'std_shared', 'var_shared',\n",
       "       'min_shared', 'max_shared', 'mean_shared', 'Child_Safety_Concern',\n",
       "       'Conflict_concerns_policy_flag', 'Engagement_consistency_positive',\n",
       "       'Mental_health_growth_positive', 'Sense_of_Growth',\n",
       "       'Relationship_quality_positive', 'Unresponsiveness_Mentioned',\n",
       "       'External_Stressors_Mentioned', 'Conflict_Language_Mentioned',\n",
       "       'Future_Oriented_Language', 'Shared_Interest',\n",
       "       'Communication_Support_Positive', 'Conflict_concerns_policy_flag_prop',\n",
       "       'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', 'Big County',\n",
       "       'Big Age', 'Big Gender', 'Big Race Category', 'Big Occupation Category',\n",
       "       'Little Gender', 'Little Race Category', 'Little Closure Age',\n",
       "       'Closure Reason Category', 'Closure Reason', 'Stage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39f298d5-b051-49f5-bb11-78e6128b377e",
   "metadata": {
    "id": "39f298d5-b051-49f5-bb11-78e6128b377e"
   },
   "outputs": [],
   "source": [
    "top_data.to_excel('Final_Topics_New.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24928443-21e5-4f7e-9082-d4f6dd9bd6e2",
   "metadata": {
    "id": "24928443-21e5-4f7e-9082-d4f6dd9bd6e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match ID 18Char</th>\n",
       "      <th>avg_call_frequency_days</th>\n",
       "      <th>std_days</th>\n",
       "      <th>var_days</th>\n",
       "      <th>min_days</th>\n",
       "      <th>max_days</th>\n",
       "      <th>mean_days</th>\n",
       "      <th>std_shared</th>\n",
       "      <th>var_shared</th>\n",
       "      <th>min_shared</th>\n",
       "      <th>...</th>\n",
       "      <th>Big Age</th>\n",
       "      <th>Big Gender</th>\n",
       "      <th>Big Race Category</th>\n",
       "      <th>Big Occupation Category</th>\n",
       "      <th>Little Gender</th>\n",
       "      <th>Little Race Category</th>\n",
       "      <th>Little Closure Age</th>\n",
       "      <th>Closure Reason Category</th>\n",
       "      <th>Closure Reason</th>\n",
       "      <th>Stage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1v2J0000027CWYQA2</td>\n",
       "      <td>3</td>\n",
       "      <td>26.501572</td>\n",
       "      <td>702.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Military and Protective Services</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11.901437</td>\n",
       "      <td>Successes</td>\n",
       "      <td>Child: Graduated</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a1v2J0000027CWfQAM</td>\n",
       "      <td>6</td>\n",
       "      <td>23.189797</td>\n",
       "      <td>537.766667</td>\n",
       "      <td>16.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>46.166667</td>\n",
       "      <td>0.547723</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>Business/Management</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>17.820671</td>\n",
       "      <td>Miscommunications</td>\n",
       "      <td>Child: Lost interest</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a1v2J0000027CWiQAM</td>\n",
       "      <td>3</td>\n",
       "      <td>25.514702</td>\n",
       "      <td>651.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Business/Management</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>12.131417</td>\n",
       "      <td>Successes</td>\n",
       "      <td>Child: Graduated</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a1v2J0000027CWoQAM</td>\n",
       "      <td>5</td>\n",
       "      <td>37.825917</td>\n",
       "      <td>1430.800000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>43.400000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Business/Management</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19.496235</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Child/Family: Moved</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a1v2J0000027CWpQAM</td>\n",
       "      <td>5</td>\n",
       "      <td>27.721833</td>\n",
       "      <td>768.500000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>Business/Management</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>14.828200</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Child: Changed school/site</td>\n",
       "      <td>Closed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Match ID 18Char  avg_call_frequency_days   std_days     var_days  \\\n",
       "0  a1v2J0000027CWYQA2                        3  26.501572   702.333333   \n",
       "1  a1v2J0000027CWfQAM                        6  23.189797   537.766667   \n",
       "2  a1v2J0000027CWiQAM                        3  25.514702   651.000000   \n",
       "3  a1v2J0000027CWoQAM                        5  37.825917  1430.800000   \n",
       "4  a1v2J0000027CWpQAM                        5  27.721833   768.500000   \n",
       "\n",
       "   min_days  max_days  mean_days  std_shared  var_shared  min_shared  ...  \\\n",
       "0       7.0      56.0  37.333333    1.000000    1.000000           0  ...   \n",
       "1      16.0      83.0  46.166667    0.547723    0.300000           0  ...   \n",
       "2      14.0      65.0  40.000000    1.527525    2.333333           0  ...   \n",
       "3      12.0     106.0  43.400000    0.447214    0.200000           0  ...   \n",
       "4      13.0      85.0  45.000000    0.894427    0.800000           0  ...   \n",
       "\n",
       "   Big Age  Big Gender  Big Race Category           Big Occupation Category  \\\n",
       "0       25      Female              Asian  Military and Protective Services   \n",
       "1       26      Female              White               Business/Management   \n",
       "2       27      Female              Asian               Business/Management   \n",
       "3       25      Female              Asian               Business/Management   \n",
       "4       27        Male              White               Business/Management   \n",
       "\n",
       "   Little Gender  Little Race Category  Little Closure Age  \\\n",
       "0         Female                 Asian           11.901437   \n",
       "1         Female                 Black           17.820671   \n",
       "2         Female                 Asian           12.131417   \n",
       "3         Female                 Asian           19.496235   \n",
       "4           Male                 Black           14.828200   \n",
       "\n",
       "   Closure Reason Category              Closure Reason   Stage  \n",
       "0                Successes            Child: Graduated  Closed  \n",
       "1        Miscommunications        Child: Lost interest  Closed  \n",
       "2                Successes            Child: Graduated  Closed  \n",
       "3                  Neutral         Child/Family: Moved  Closed  \n",
       "4                  Neutral  Child: Changed school/site  Closed  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14502208-a342-4bb6-8c34-29894f3501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Filter only topic columns and closure reason\n",
    "topic_cols = [col for col in merged_df.columns if col.startswith(\"Topic_\")]\n",
    "plot_df = merged_df[topic_cols + ['Closure Reason Category']]\n",
    "\n",
    "# Step 2: Group by closure reason and calculate mean topic distribution\n",
    "grouped_means = plot_df.groupby('Closure Reason Category').mean()\n",
    "\n",
    "# Step 3: Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(grouped_means, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={\"label\": \"Mean Topic Probability\"})\n",
    "\n",
    "plt.title(\"📊 Mean Topic Distribution by Closure Reason Category\")\n",
    "plt.ylabel(\"Closure Reason Category\")\n",
    "plt.xlabel(\"Topics\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb711847-c297-4a16-9d6c-7571d3d09c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
